{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EEG-Emotions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOfnNT/BI6t+3Ew+jNTW+Be",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdamFulton/COMP3100StageTwo/blob/main/EEG_Emotions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 319,
      "metadata": {
        "id": "cJN9im36VIdb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader,SubsetRandomSampler\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pickle as pickle\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNkgPatpVQAZ",
        "outputId": "cc3cb57f-9789-4833-c454-be74d436d95b"
      },
      "execution_count": 320,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat May 14 05:51:19 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    33W / 250W |   8707MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3J5bfscsVQvi",
        "outputId": "3b472d38-a8c2-476a-87ed-316323d0e206"
      },
      "execution_count": 321,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_eeg_signal_from_file(filename):\n",
        "    x = pickle._Unpickler(open(filename, 'rb'))\n",
        "    x.encoding = 'latin1'\n",
        "    p = x.load()\n",
        "    return p\n",
        "\n"
      ],
      "metadata": {
        "id": "YfbW8rTXVhs5"
      },
      "execution_count": 322,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files = []\n",
        "for n in range(1, 33): \n",
        "    s = ''\n",
        "    if n < 10:\n",
        "        s += '0'\n",
        "    s += str(n)\n",
        "    files.append(s)\n",
        "print(files)"
      ],
      "metadata": {
        "id": "luuB-fKL-YJK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43400ca1-6628-4547-a6f7-08da15637e11"
      },
      "execution_count": 323,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "labels = []\n",
        "data = []\n",
        "\n",
        "for i in files: \n",
        "    filename = \"/content/gdrive/MyDrive/data_preprocessed_python/s\" + i + \".dat\"\n",
        "    trial = read_eeg_signal_from_file(filename)\n",
        "    labels.append(trial['labels'])\n",
        "    data.append(trial['data'])\n",
        "\n",
        "labels = np.array(labels)\n",
        "labels = labels.flatten()\n",
        "labels = labels.reshape(1280, 4)\n",
        "\n",
        "data = np.array(data)\n",
        "data = data.flatten()\n",
        "data = data.reshape(1280, 40, 8064)"
      ],
      "metadata": {
        "id": "J9bAG7DT-ZtL"
      },
      "execution_count": 324,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Labels: \", labels.shape)\n",
        "print(\"Data: \", data.shape)"
      ],
      "metadata": {
        "id": "Ue0LuHpU-bzn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e97ab467-fc98-486c-d539-35fa0ce3e2c4"
      },
      "execution_count": 325,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels:  (1280, 4)\n",
            "Data:  (1280, 40, 8064)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_label_ratings = pd.DataFrame({'Valence': labels[:,0], 'Arousal': labels[:,1]})\n",
        "print(df_label_ratings.describe())"
      ],
      "metadata": {
        "id": "mfASJG9k-dUz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4b4ad34-e298-4c48-e446-851a2d4ae88b"
      },
      "execution_count": 326,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           Valence      Arousal\n",
            "count  1280.000000  1280.000000\n",
            "mean      5.254313     5.156711\n",
            "std       2.130816     2.020499\n",
            "min       1.000000     1.000000\n",
            "25%       3.867500     3.762500\n",
            "50%       5.040000     5.230000\n",
            "75%       7.050000     6.950000\n",
            "max       9.000000     9.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_label_ratings.head(15))"
      ],
      "metadata": {
        "id": "acmGHCxF-dF8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15183545-39da-4d16-d840-65c2a4883333"
      },
      "execution_count": 329,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Valence  Arousal\n",
            "0      7.71     7.60\n",
            "1      8.10     7.31\n",
            "2      8.58     7.54\n",
            "3      4.94     6.01\n",
            "4      6.96     3.92\n",
            "5      8.27     3.92\n",
            "6      7.44     3.73\n",
            "7      7.32     2.55\n",
            "8      4.04     3.29\n",
            "9      1.99     4.86\n",
            "10     2.99     2.36\n",
            "11     2.71     2.77\n",
            "12     1.95     3.12\n",
            "13     4.18     2.24\n",
            "14     3.17     8.08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_hahv = df_label_ratings[(df_label_ratings['Valence'] >= np.median(labels[:,0])) & (df_label_ratings['Arousal'] >= np.median(labels[:,1]))]\n",
        "\n",
        "df_lahv = df_label_ratings[(df_label_ratings['Valence'] >= np.median(labels[:,0])) & (df_label_ratings['Arousal'] < np.median(labels[:,1]))]\n",
        "\n",
        "df_halv = df_label_ratings[(df_label_ratings['Valence'] < np.median(labels[:,0])) & (df_label_ratings['Arousal'] >= np.median(labels[:,1]))]\n",
        "\n",
        "df_lalv = df_label_ratings[(df_label_ratings['Valence'] < np.median(labels[:,0])) & (df_label_ratings['Arousal'] < np.median(labels[:,1]))]"
      ],
      "metadata": {
        "id": "F6iHWrSg-ofL"
      },
      "execution_count": 330,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Positive Valence:\", str(len(df_hahv) + len(df_lahv)))\n",
        "print(\"Negative Valence:\", str(len(df_halv) + len(df_lalv)))\n",
        "print(\"High Arousal:\", str(len(df_hahv) + len(df_halv)))\n",
        "print(\"Low Arousal:\", str(len(df_lahv) + len(df_lalv)))"
      ],
      "metadata": {
        "id": "tHINl7qh-ubS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65cd0f89-f20b-442c-ee17-d4ee540ca591"
      },
      "execution_count": 331,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive Valence: 680\n",
            "Negative Valence: 600\n",
            "High Arousal: 640\n",
            "Low Arousal: 640\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def positive_valence(trial):\n",
        "    return 1 if labels[trial,0] >= np.median(labels[:,0]) else 0 \n",
        "\n",
        "def high_arousal(trial):\n",
        "    return 1 if labels[trial,1] >= np.median(labels[:,1]) else 0"
      ],
      "metadata": {
        "id": "AaJabfbq-v6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_encoded = []\n",
        "for i in range (len(labels)):\n",
        "  labels_encoded.append([positive_valence(i), high_arousal(i)])\n",
        "\n",
        "labels_encoded = np.reshape(labels_encoded, (1280, 2))\n",
        "df_labels = pd.DataFrame(data=labels_encoded, columns=[\"Positive Valence\", \"High Arousal\",])\n",
        "\n"
      ],
      "metadata": {
        "id": "ySmfjKk6-xoF"
      },
      "execution_count": 332,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_valence = df_labels['Positive Valence']\n",
        "\n",
        "df_arousal = df_labels['High Arousal']\n",
        "\n",
        "df_arousal = df_arousal.to_numpy()\n",
        "\n",
        "df_valence = df_valence.to_numpy()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1wcAUsAQ-3TA"
      },
      "execution_count": 333,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eeg_channels = np.array([\"Fp1\", \"AF3\", \"F3\", \"F7\", \"FC5\", \"FC1\", \"C3\", \"T7\", \"CP5\", \"CP1\", \"P3\", \"P7\", \"PO3\", \"O1\", \"Oz\", \"Pz\", \"Fp2\", \"AF4\", \"Fz\", \"F4\", \"F8\", \"FC6\", \"FC2\", \"Cz\", \"C4\", \"T8\", \"CP6\", \"CP2\", \"P4\", \"P8\", \"PO4\", \"O2\"])\n",
        "peripheral_channels = np.array([\"hEOG\", \"vEOG\", \"zEMG\", \"tEMG\", \"GSR\", \"Respiration belt\", \"Plethysmograph\", \"Temperature\"])\n"
      ],
      "metadata": {
        "id": "k42Z2F-74egK"
      },
      "execution_count": 334,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eeg_data = []\n",
        "for i in range (len(data)):\n",
        "    for j in range (len(eeg_channels)):\n",
        "        eeg_data.append(data[i,j])\n",
        "eeg_data = np.reshape(eeg_data, (len(data), len(eeg_channels), len(data[0,0])))\n",
        "print(eeg_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Xsc1bSs4g56",
        "outputId": "bfd7887b-bfaa-4ccf-a2fc-e2ba0f11a090"
      },
      "execution_count": 335,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1280, 32, 8064)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(0.8 * len(eeg_data))\n",
        "valid_size = len(eeg_data) - train_size\n",
        "train_dataset, valid_dataset = torch.utils.data.random_split(eeg_data, [train_size, valid_size])\n",
        "train_dataset = np.array(train_dataset)\n",
        "valid_dataset = np.array(valid_dataset)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(train_dataset.reshape(-1, train_dataset.shape[-1])).reshape(train_dataset.shape)\n",
        "X_test = scaler.transform(valid_dataset.reshape(-1, valid_dataset.shape[-1])).reshape(valid_dataset.shape)\n",
        "\n",
        "\n",
        "\n",
        "train = []\n",
        "valid = []\n",
        "print(len(X_test))\n",
        "for i in range(valid_size,train_size):\n",
        "\n",
        "    valid.append((X_train[i], df_valence[i]))\n",
        "  \n",
        "\n",
        "for i in range(0,len(X_test)):\n",
        "\n",
        "    train.append((X_test[i],df_valence[i]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkqW9zjxAWsP",
        "outputId": "ee3f338b-b7c8-4ad3-a85f-47772f7a02ad"
      },
      "execution_count": 336,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "256\n",
            "(array([[-1.26700192, -0.69311858, -0.46014445, ..., -0.22123851,\n",
            "        -0.47546754, -0.28220913],\n",
            "       [-1.80193374, -0.84759446, -0.13018708, ...,  0.08564208,\n",
            "        -0.51966667, -0.44539885],\n",
            "       [-1.25159991, -0.34756283,  0.7239788 , ...,  0.07629579,\n",
            "        -0.63620148, -0.55901563],\n",
            "       ...,\n",
            "       [-1.62262069, -0.57195197,  0.85083543, ..., -0.00479637,\n",
            "        -1.01674883, -0.71999102],\n",
            "       [-1.69913995, -0.63997153,  0.76780417, ...,  0.0590668 ,\n",
            "        -1.11963866, -0.87837184],\n",
            "       [-2.12438508, -0.78088265,  1.01784947, ..., -0.00246079,\n",
            "        -1.24424848, -0.97398621]]), 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=16\n",
        "\n",
        "\n",
        "train_dl = DataLoader(train,batch_size)\n",
        "\n",
        "\n",
        "valid_dl = DataLoader(valid,batch_size)\n",
        "\n",
        "\n",
        "len(train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KOpHJXTaYy-",
        "outputId": "7667dfef-7728-4c51-95aa-ca4d5bc476e7"
      },
      "execution_count": 337,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "256"
            ]
          },
          "metadata": {},
          "execution_count": 337
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        def conv_bn(inp, oup, stride):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv1d(inp, oup, 2, stride, 1, bias=False),\n",
        "                nn.BatchNorm1d(oup),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Dropout(p=0.25),\n",
        "            )\n",
        "\n",
        "        def conv_dw(inp, oup, stride):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv1d(inp, inp, 2, stride, 1, groups=inp, bias=False),\n",
        "                nn.BatchNorm1d(inp),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Dropout(p=0.25),\n",
        "                \n",
        "              \n",
        "    \n",
        "                nn.Conv1d(inp, oup, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm1d(oup),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Dropout(p=0.25),\n",
        "               \n",
        "               \n",
        "            )\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            conv_bn( 32,  128, 2), \n",
        "            conv_dw( 128,  200, 1),\n",
        "            conv_dw( 200, 225, 2),\n",
        "            conv_dw(225, 245, 1),\n",
        "            conv_dw(245, 256, 2),\n",
        "            conv_dw(256, 412, 1),\n",
        "            conv_dw(412, 512, 2),\n",
        "            conv_dw(512, 512, 1),\n",
        "            nn.AvgPool1d(260),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "        nn.Linear(512, 512),\n",
        "      \tnn.ReLU(),\n",
        "        nn.Dropout(p=0.75),\n",
        "        nn.Linear(512, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(p=0.75),\n",
        "        nn.Linear(256, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(p=0.75),\n",
        "        nn.Linear(128, 64),\n",
        "        nn.Softmax(),\n",
        "\n",
        "        nn.Linear(64,2)\n",
        "        \n",
        "\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        x = x.view(-1, 512)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "D_jZvIcOb8n7"
      },
      "execution_count": 338,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_default_device():\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "    return torch.device(\"cuda\")\n",
        "\n",
        "  else:\n",
        "    return torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "yhWdCylqcCaL"
      },
      "execution_count": 339,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = get_default_device();\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbNQ80gIWapv",
        "outputId": "c6731927-dfc8-4d0e-ae45-2f4a2e818310"
      },
      "execution_count": 340,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 340
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def to_device(data,device):\n",
        "\n",
        "  if isinstance(data,(list,tuple)):\n",
        "    return [to_device(x,device) for x in data]\n",
        "\n",
        "  return data.to(device,non_blocking=True)"
      ],
      "metadata": {
        "id": "B_iltgDnWbgM"
      },
      "execution_count": 341,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for data,labels in train_dl:\n",
        "  print(data.shape)\n",
        "  data = to_device(data,device)\n",
        "  print(data.device)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jNuP1TxWdug",
        "outputId": "2d068902-4383-489e-afbf-4a4feecf57c8"
      },
      "execution_count": 342,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 32, 8064])\n",
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DeviceDataLoader():\n",
        "    \n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "        \n",
        "    def __iter__(self):\n",
        "        \n",
        "        for b in self.dl: \n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "       \n",
        "        return len(self.dl)"
      ],
      "metadata": {
        "id": "iNr5k3NgWtaK"
      },
      "execution_count": 343,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dl  = DeviceDataLoader(train_dl,device)\n",
        "valid_dl = DeviceDataLoader(valid_dl,device)\n",
        "model = Net()\n",
        "to_device(model,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZSMNplsWuFW",
        "outputId": "0e0d8aac-d103-4c67-c1e4-088b7ae1d15b"
      },
      "execution_count": 344,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (model): Sequential(\n",
              "    (0): Sequential(\n",
              "      (0): Conv1d(32, 128, kernel_size=(2,), stride=(2,), padding=(1,), bias=False)\n",
              "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Dropout(p=0.25, inplace=False)\n",
              "    )\n",
              "    (1): Sequential(\n",
              "      (0): Conv1d(128, 128, kernel_size=(2,), stride=(1,), padding=(1,), groups=128, bias=False)\n",
              "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Dropout(p=0.25, inplace=False)\n",
              "      (4): Conv1d(128, 200, kernel_size=(1,), stride=(1,), bias=False)\n",
              "      (5): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (6): ReLU(inplace=True)\n",
              "      (7): Dropout(p=0.25, inplace=False)\n",
              "    )\n",
              "    (2): Sequential(\n",
              "      (0): Conv1d(200, 200, kernel_size=(2,), stride=(2,), padding=(1,), groups=200, bias=False)\n",
              "      (1): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Dropout(p=0.25, inplace=False)\n",
              "      (4): Conv1d(200, 225, kernel_size=(1,), stride=(1,), bias=False)\n",
              "      (5): BatchNorm1d(225, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (6): ReLU(inplace=True)\n",
              "      (7): Dropout(p=0.25, inplace=False)\n",
              "    )\n",
              "    (3): Sequential(\n",
              "      (0): Conv1d(225, 225, kernel_size=(2,), stride=(1,), padding=(1,), groups=225, bias=False)\n",
              "      (1): BatchNorm1d(225, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Dropout(p=0.25, inplace=False)\n",
              "      (4): Conv1d(225, 245, kernel_size=(1,), stride=(1,), bias=False)\n",
              "      (5): BatchNorm1d(245, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (6): ReLU(inplace=True)\n",
              "      (7): Dropout(p=0.25, inplace=False)\n",
              "    )\n",
              "    (4): Sequential(\n",
              "      (0): Conv1d(245, 245, kernel_size=(2,), stride=(2,), padding=(1,), groups=245, bias=False)\n",
              "      (1): BatchNorm1d(245, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Dropout(p=0.25, inplace=False)\n",
              "      (4): Conv1d(245, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
              "      (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (6): ReLU(inplace=True)\n",
              "      (7): Dropout(p=0.25, inplace=False)\n",
              "    )\n",
              "    (5): Sequential(\n",
              "      (0): Conv1d(256, 256, kernel_size=(2,), stride=(1,), padding=(1,), groups=256, bias=False)\n",
              "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Dropout(p=0.25, inplace=False)\n",
              "      (4): Conv1d(256, 412, kernel_size=(1,), stride=(1,), bias=False)\n",
              "      (5): BatchNorm1d(412, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (6): ReLU(inplace=True)\n",
              "      (7): Dropout(p=0.25, inplace=False)\n",
              "    )\n",
              "    (6): Sequential(\n",
              "      (0): Conv1d(412, 412, kernel_size=(2,), stride=(2,), padding=(1,), groups=412, bias=False)\n",
              "      (1): BatchNorm1d(412, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Dropout(p=0.25, inplace=False)\n",
              "      (4): Conv1d(412, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
              "      (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (6): ReLU(inplace=True)\n",
              "      (7): Dropout(p=0.25, inplace=False)\n",
              "    )\n",
              "    (7): Sequential(\n",
              "      (0): Conv1d(512, 512, kernel_size=(2,), stride=(1,), padding=(1,), groups=512, bias=False)\n",
              "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Dropout(p=0.25, inplace=False)\n",
              "      (4): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
              "      (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (6): ReLU(inplace=True)\n",
              "      (7): Dropout(p=0.25, inplace=False)\n",
              "    )\n",
              "    (8): AvgPool1d(kernel_size=(260,), stride=(260,), padding=(0,))\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Dropout(p=0.75, inplace=False)\n",
              "    (3): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (4): ReLU()\n",
              "    (5): Dropout(p=0.75, inplace=False)\n",
              "    (6): Linear(in_features=256, out_features=128, bias=True)\n",
              "    (7): ReLU()\n",
              "    (8): Dropout(p=0.75, inplace=False)\n",
              "    (9): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (10): Softmax(dim=None)\n",
              "    (11): Linear(in_features=64, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 344
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(outputs,labels):\n",
        "    _, preds = torch.max(outputs,dim=1)\n",
        "    return torch.sum(preds == labels).item() / len(preds)"
      ],
      "metadata": {
        "id": "ze5MfUci47hf"
      },
      "execution_count": 345,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "val_loss, _,val_acc =evaluate(model,nn.CrossEntropyLoss(),valid_dl,metric=accuracy)\n",
        "print('Loss: {:.4f}, Accuracy: {:.4f}'.format(val_loss,val_acc))"
      ],
      "metadata": {
        "id": "kAqGuyTzB6mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = Net().to(device)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "min_valid_loss = np.inf"
      ],
      "metadata": {
        "id": "wrgbVYbeW4TN"
      },
      "execution_count": 346,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "for e in range(100):\n",
        "    train_loss = 0.0\n",
        "    model.train()     \n",
        "    for data, labels in train_dl:\n",
        "        if torch.cuda.is_available():\n",
        "            data, labels = data.cuda(), labels.cuda()\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        data = data.float()\n",
        "        target = model(data)\n",
        "        loss = loss_function(target,labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "    \n",
        "    valid_loss = 0.0\n",
        "    model.eval()     \n",
        "    for data, labels in valid_dl:\n",
        "        if torch.cuda.is_available():\n",
        "            data, labels = data.cuda(), labels.cuda()\n",
        "        data = data.float()\n",
        "        target = model(data)\n",
        "        loss = loss_function(target,labels)\n",
        "        valid_loss = loss.item() * data.size(0)\n",
        "\n",
        "    print(f'Epoch {e+1} \\t\\t Training Loss: {train_loss / len(train_dl)} \\t\\t Validation Loss: {valid_loss / len(valid_dl)}')\n",
        "    if min_valid_loss > valid_loss:\n",
        "        print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f}) \\t Saving The Model')\n",
        "        min_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_model.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SibKmDpNW8Jg",
        "outputId": "ec032114-c5bd-4062-ad02-ad192b2cfda9"
      },
      "execution_count": 347,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 \t\t Training Loss: 0.6894524171948433 \t\t Validation Loss: 0.22324277957280478\n",
            "Validation Loss Decreased(inf--->10.715653) \t Saving The Model\n",
            "Epoch 2 \t\t Training Loss: 0.6893130801618099 \t\t Validation Loss: 0.22306295235951742\n",
            "Validation Loss Decreased(10.715653--->10.707022) \t Saving The Model\n",
            "Epoch 3 \t\t Training Loss: 0.6892875544726849 \t\t Validation Loss: 0.22287797927856445\n",
            "Validation Loss Decreased(10.707022--->10.698143) \t Saving The Model\n",
            "Epoch 4 \t\t Training Loss: 0.6892017722129822 \t\t Validation Loss: 0.22268142302831015\n",
            "Validation Loss Decreased(10.698143--->10.688708) \t Saving The Model\n",
            "Epoch 5 \t\t Training Loss: 0.689043965190649 \t\t Validation Loss: 0.22248599926630655\n",
            "Validation Loss Decreased(10.688708--->10.679328) \t Saving The Model\n",
            "Epoch 6 \t\t Training Loss: 0.6888999566435814 \t\t Validation Loss: 0.2222959597905477\n",
            "Validation Loss Decreased(10.679328--->10.670206) \t Saving The Model\n",
            "Epoch 7 \t\t Training Loss: 0.6889811083674431 \t\t Validation Loss: 0.22210816542307535\n",
            "Validation Loss Decreased(10.670206--->10.661192) \t Saving The Model\n",
            "Epoch 8 \t\t Training Loss: 0.6886616498231888 \t\t Validation Loss: 0.22193135817845663\n",
            "Validation Loss Decreased(10.661192--->10.652705) \t Saving The Model\n",
            "Epoch 9 \t\t Training Loss: 0.6885993033647537 \t\t Validation Loss: 0.22174155712127686\n",
            "Validation Loss Decreased(10.652705--->10.643595) \t Saving The Model\n",
            "Epoch 10 \t\t Training Loss: 0.6885500065982342 \t\t Validation Loss: 0.22154957056045532\n",
            "Validation Loss Decreased(10.643595--->10.634379) \t Saving The Model\n",
            "Epoch 11 \t\t Training Loss: 0.6882436163723469 \t\t Validation Loss: 0.22135388851165771\n",
            "Validation Loss Decreased(10.634379--->10.624987) \t Saving The Model\n",
            "Epoch 12 \t\t Training Loss: 0.6883296482264996 \t\t Validation Loss: 0.221169114112854\n",
            "Validation Loss Decreased(10.624987--->10.616117) \t Saving The Model\n",
            "Epoch 13 \t\t Training Loss: 0.6883116476237774 \t\t Validation Loss: 0.22099224726359049\n",
            "Validation Loss Decreased(10.616117--->10.607628) \t Saving The Model\n",
            "Epoch 14 \t\t Training Loss: 0.6882055252790451 \t\t Validation Loss: 0.2208064397176107\n",
            "Validation Loss Decreased(10.607628--->10.598709) \t Saving The Model\n",
            "Epoch 15 \t\t Training Loss: 0.6879930235445499 \t\t Validation Loss: 0.2206315795580546\n",
            "Validation Loss Decreased(10.598709--->10.590316) \t Saving The Model\n",
            "Epoch 16 \t\t Training Loss: 0.6879205405712128 \t\t Validation Loss: 0.22045381863911948\n",
            "Validation Loss Decreased(10.590316--->10.581783) \t Saving The Model\n",
            "Epoch 17 \t\t Training Loss: 0.6873773746192455 \t\t Validation Loss: 0.22027246157328287\n",
            "Validation Loss Decreased(10.581783--->10.573078) \t Saving The Model\n",
            "Epoch 18 \t\t Training Loss: 0.6869694888591766 \t\t Validation Loss: 0.22009829680124918\n",
            "Validation Loss Decreased(10.573078--->10.564718) \t Saving The Model\n",
            "Epoch 19 \t\t Training Loss: 0.6869777143001556 \t\t Validation Loss: 0.2199124296506246\n",
            "Validation Loss Decreased(10.564718--->10.555797) \t Saving The Model\n",
            "Epoch 20 \t\t Training Loss: 0.6867892667651176 \t\t Validation Loss: 0.21971027056376138\n",
            "Validation Loss Decreased(10.555797--->10.546093) \t Saving The Model\n",
            "Epoch 21 \t\t Training Loss: 0.6861888691782951 \t\t Validation Loss: 0.21950831015904745\n",
            "Validation Loss Decreased(10.546093--->10.536399) \t Saving The Model\n",
            "Epoch 22 \t\t Training Loss: 0.6866736225783825 \t\t Validation Loss: 0.21940036614735922\n",
            "Validation Loss Decreased(10.536399--->10.531218) \t Saving The Model\n",
            "Epoch 23 \t\t Training Loss: 0.6854768209159374 \t\t Validation Loss: 0.21929222345352173\n",
            "Validation Loss Decreased(10.531218--->10.526027) \t Saving The Model\n",
            "Epoch 24 \t\t Training Loss: 0.6859221272170544 \t\t Validation Loss: 0.2191432317097982\n",
            "Validation Loss Decreased(10.526027--->10.518875) \t Saving The Model\n",
            "Epoch 25 \t\t Training Loss: 0.6849680915474892 \t\t Validation Loss: 0.21898160378138223\n",
            "Validation Loss Decreased(10.518875--->10.511117) \t Saving The Model\n",
            "Epoch 26 \t\t Training Loss: 0.6857052966952324 \t\t Validation Loss: 0.21882164478302002\n",
            "Validation Loss Decreased(10.511117--->10.503439) \t Saving The Model\n",
            "Epoch 27 \t\t Training Loss: 0.6840609312057495 \t\t Validation Loss: 0.21868417660395303\n",
            "Validation Loss Decreased(10.503439--->10.496840) \t Saving The Model\n",
            "Epoch 28 \t\t Training Loss: 0.6836107075214386 \t\t Validation Loss: 0.21859681606292725\n",
            "Validation Loss Decreased(10.496840--->10.492647) \t Saving The Model\n",
            "Epoch 29 \t\t Training Loss: 0.6827278546988964 \t\t Validation Loss: 0.2185049851735433\n",
            "Validation Loss Decreased(10.492647--->10.488239) \t Saving The Model\n",
            "Epoch 30 \t\t Training Loss: 0.6825516484677792 \t\t Validation Loss: 0.21843401590983072\n",
            "Validation Loss Decreased(10.488239--->10.484833) \t Saving The Model\n",
            "Epoch 31 \t\t Training Loss: 0.6813037879765034 \t\t Validation Loss: 0.2183629870414734\n",
            "Validation Loss Decreased(10.484833--->10.481423) \t Saving The Model\n",
            "Epoch 32 \t\t Training Loss: 0.6805440485477448 \t\t Validation Loss: 0.2183207074801127\n",
            "Validation Loss Decreased(10.481423--->10.479394) \t Saving The Model\n",
            "Epoch 33 \t\t Training Loss: 0.6794058792293072 \t\t Validation Loss: 0.21832942962646484\n",
            "Epoch 34 \t\t Training Loss: 0.6772650294005871 \t\t Validation Loss: 0.21835434436798096\n",
            "Epoch 35 \t\t Training Loss: 0.6770432591438293 \t\t Validation Loss: 0.21837260325749716\n",
            "Epoch 36 \t\t Training Loss: 0.6769123561680317 \t\t Validation Loss: 0.21837989489237467\n",
            "Epoch 37 \t\t Training Loss: 0.6761103868484497 \t\t Validation Loss: 0.2184845209121704\n",
            "Epoch 38 \t\t Training Loss: 0.6746152378618717 \t\t Validation Loss: 0.21850425004959106\n",
            "Epoch 39 \t\t Training Loss: 0.6717105619609356 \t\t Validation Loss: 0.21859526634216309\n",
            "Epoch 40 \t\t Training Loss: 0.6698332615196705 \t\t Validation Loss: 0.21864362557729086\n",
            "Epoch 41 \t\t Training Loss: 0.6723102293908596 \t\t Validation Loss: 0.21869152784347534\n",
            "Epoch 42 \t\t Training Loss: 0.6696028336882591 \t\t Validation Loss: 0.21880996227264404\n",
            "Epoch 43 \t\t Training Loss: 0.6677493378520012 \t\t Validation Loss: 0.21890681982040405\n",
            "Epoch 44 \t\t Training Loss: 0.6645095981657505 \t\t Validation Loss: 0.21900163094202676\n",
            "Epoch 45 \t\t Training Loss: 0.6645793467760086 \t\t Validation Loss: 0.21912405888239542\n",
            "Epoch 46 \t\t Training Loss: 0.6635800376534462 \t\t Validation Loss: 0.21924229462941489\n",
            "Epoch 47 \t\t Training Loss: 0.6606447845697403 \t\t Validation Loss: 0.21937918663024902\n",
            "Epoch 48 \t\t Training Loss: 0.6606581211090088 \t\t Validation Loss: 0.21952478090922037\n",
            "Epoch 49 \t\t Training Loss: 0.6583837941288948 \t\t Validation Loss: 0.21966314315795898\n",
            "Epoch 50 \t\t Training Loss: 0.6595126651227474 \t\t Validation Loss: 0.2198119560877482\n",
            "Epoch 51 \t\t Training Loss: 0.6557302102446556 \t\t Validation Loss: 0.21995919942855835\n",
            "Epoch 52 \t\t Training Loss: 0.65384541451931 \t\t Validation Loss: 0.2201117475827535\n",
            "Epoch 53 \t\t Training Loss: 0.654736191034317 \t\t Validation Loss: 0.22026397784550986\n",
            "Epoch 54 \t\t Training Loss: 0.6533929258584976 \t\t Validation Loss: 0.22043377161026\n",
            "Epoch 55 \t\t Training Loss: 0.6513075567781925 \t\t Validation Loss: 0.22059633334477743\n",
            "Epoch 56 \t\t Training Loss: 0.649116475135088 \t\t Validation Loss: 0.22077288230260214\n",
            "Epoch 57 \t\t Training Loss: 0.6469093337655067 \t\t Validation Loss: 0.2209581732749939\n",
            "Epoch 58 \t\t Training Loss: 0.6488816812634468 \t\t Validation Loss: 0.2211515506108602\n",
            "Epoch 59 \t\t Training Loss: 0.6444743536412716 \t\t Validation Loss: 0.22134419282277426\n",
            "Epoch 60 \t\t Training Loss: 0.6454587653279305 \t\t Validation Loss: 0.22154506047566733\n",
            "Epoch 61 \t\t Training Loss: 0.6416287384927273 \t\t Validation Loss: 0.22175081570943198\n",
            "Epoch 62 \t\t Training Loss: 0.6434799693524837 \t\t Validation Loss: 0.22196086247762045\n",
            "Epoch 63 \t\t Training Loss: 0.6411672756075859 \t\t Validation Loss: 0.22216459115346274\n",
            "Epoch 64 \t\t Training Loss: 0.6394288092851639 \t\t Validation Loss: 0.22237839301427206\n",
            "Epoch 65 \t\t Training Loss: 0.637960497289896 \t\t Validation Loss: 0.22258772452672324\n",
            "Epoch 66 \t\t Training Loss: 0.6330685764551163 \t\t Validation Loss: 0.2228169043858846\n",
            "Epoch 67 \t\t Training Loss: 0.6336428746581078 \t\t Validation Loss: 0.2230465809504191\n",
            "Epoch 68 \t\t Training Loss: 0.6327636055648327 \t\t Validation Loss: 0.22328893343607584\n",
            "Epoch 69 \t\t Training Loss: 0.6333634816110134 \t\t Validation Loss: 0.2235331137975057\n",
            "Epoch 70 \t\t Training Loss: 0.6346484459936619 \t\t Validation Loss: 0.22377715508143106\n",
            "Epoch 71 \t\t Training Loss: 0.6302482038736343 \t\t Validation Loss: 0.2240204413731893\n",
            "Epoch 72 \t\t Training Loss: 0.6299575679004192 \t\t Validation Loss: 0.22427175442377725\n",
            "Epoch 73 \t\t Training Loss: 0.6265517435967922 \t\t Validation Loss: 0.22452308734258017\n",
            "Epoch 74 \t\t Training Loss: 0.6265038475394249 \t\t Validation Loss: 0.2247934341430664\n",
            "Epoch 75 \t\t Training Loss: 0.6246153935790062 \t\t Validation Loss: 0.22508060932159424\n",
            "Epoch 76 \t\t Training Loss: 0.6220318004488945 \t\t Validation Loss: 0.225368599096934\n",
            "Epoch 77 \t\t Training Loss: 0.6214251182973385 \t\t Validation Loss: 0.22567282120386759\n",
            "Epoch 78 \t\t Training Loss: 0.6193918958306313 \t\t Validation Loss: 0.2259628971417745\n",
            "Epoch 79 \t\t Training Loss: 0.6176182888448238 \t\t Validation Loss: 0.22627872228622437\n",
            "Epoch 80 \t\t Training Loss: 0.618588674813509 \t\t Validation Loss: 0.226600448290507\n",
            "Epoch 81 \t\t Training Loss: 0.6144583746790886 \t\t Validation Loss: 0.22695585091908774\n",
            "Epoch 82 \t\t Training Loss: 0.6168112345039845 \t\t Validation Loss: 0.22730167706807455\n",
            "Epoch 83 \t\t Training Loss: 0.6149977594614029 \t\t Validation Loss: 0.2276497483253479\n",
            "Epoch 84 \t\t Training Loss: 0.6154797188937664 \t\t Validation Loss: 0.22801198561986288\n",
            "Epoch 85 \t\t Training Loss: 0.6139630377292633 \t\t Validation Loss: 0.22838791211446127\n",
            "Epoch 86 \t\t Training Loss: 0.6084246262907982 \t\t Validation Loss: 0.2288182775179545\n",
            "Epoch 87 \t\t Training Loss: 0.6106315068900585 \t\t Validation Loss: 0.22926968336105347\n",
            "Epoch 88 \t\t Training Loss: 0.6122127324342728 \t\t Validation Loss: 0.22971596320470175\n",
            "Epoch 89 \t\t Training Loss: 0.606786023825407 \t\t Validation Loss: 0.2301745613416036\n",
            "Epoch 90 \t\t Training Loss: 0.6085344962775707 \t\t Validation Loss: 0.2307529846827189\n",
            "Epoch 91 \t\t Training Loss: 0.6124747879803181 \t\t Validation Loss: 0.23136321703592935\n",
            "Epoch 92 \t\t Training Loss: 0.6080592125654221 \t\t Validation Loss: 0.23193158706029257\n",
            "Epoch 93 \t\t Training Loss: 0.6041684523224831 \t\t Validation Loss: 0.23261825243631998\n",
            "Epoch 94 \t\t Training Loss: 0.6071274988353252 \t\t Validation Loss: 0.2335099975268046\n",
            "Epoch 95 \t\t Training Loss: 0.6030494682490826 \t\t Validation Loss: 0.2343682050704956\n",
            "Epoch 96 \t\t Training Loss: 0.6064237840473652 \t\t Validation Loss: 0.23554684718449911\n",
            "Epoch 97 \t\t Training Loss: 0.6000217720866203 \t\t Validation Loss: 0.23666898409525552\n",
            "Epoch 98 \t\t Training Loss: 0.5980367790907621 \t\t Validation Loss: 0.23825947443644205\n",
            "Epoch 99 \t\t Training Loss: 0.5969586223363876 \t\t Validation Loss: 0.2401480476061503\n",
            "Epoch 100 \t\t Training Loss: 0.5895718857645988 \t\t Validation Loss: 0.24242512385050455\n"
          ]
        }
      ]
    }
  ]
}